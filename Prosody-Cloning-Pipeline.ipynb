{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Trump dataset\n",
    "from datasets import load_dataset\n",
    "trump_dataset = load_dataset(\"tuenguyen/trump-speech-dataset-tts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to sample\n",
    "from IPython.display import Audio\n",
    "trump_sample = trump_dataset[\"train\"][0]\n",
    "audio_array = trump_sample[\"path\"][\"array\"]\n",
    "sr = trump_sample[\"path\"][\"sampling_rate\"]\n",
    "audio_data = Audio(audio_array, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the audio data to a wav file\n",
    "with open(\"trump_sample.wav\", \"wb\") as opt_file:\n",
    "    opt_file.write(audio_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "Marijuana legalization presents an opportunity to promote public health, reduce crime, and generate tax revenue. \n",
    "Evidence suggests that regulation can ensure safer consumption, reduce the burden on the criminal justice system, and direct law enforcement resources toward more serious crimes. \n",
    "Additionally, the economic benefits of legal cannabis industries, including job creation and tax revenue, have been demonstrated in states where it is already legal. \n",
    "By focusing on regulation over prohibition, we can ensure a more effective and balanced approach to marijuana use and its societal impact.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tortoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#initialize TTS\n",
    "tortoise = TTS(\"tts_models/en/multi-dataset/tortoise-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0449, -0.0230,  0.1235,  ...,  0.2088,  0.2020,  0.1975],\n",
      "       dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "voice_samples = [torch.tensor(audio_array)]\n",
    "print(voice_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TTS.tts.models.tortoise.Tortoise.inference_with_config() got multiple values for keyword argument 'voice_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtortoise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_output_tortoise.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoice_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvoice_samples\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:374\u001b[39m, in \u001b[36mTTS.tts_to_file\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, pipe_out, file_path, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    346\u001b[39m \n\u001b[32m    347\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.synthesizer.save_wav(wav=wav, path=file_path, pipe_out=pipe_out)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:323\u001b[39m, in \u001b[36mTTS.tts\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m    302\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, emotion=emotion, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/utils/synthesizer.py:422\u001b[39m, in \u001b[36mSynthesizer.tts\u001b[39m\u001b[34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.tts_model, \u001b[33m\"\u001b[39m\u001b[33msynthesize\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvoice_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43md_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    433\u001b[39m         \u001b[38;5;66;03m# synthesize voice\u001b[39;00m\n\u001b[32m    434\u001b[39m         outputs = synthesis(\n\u001b[32m    435\u001b[39m             model=\u001b[38;5;28mself\u001b[39m.tts_model,\n\u001b[32m    436\u001b[39m             text=sen,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m             language_id=language_id,\n\u001b[32m    445\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/models/tortoise.py:539\u001b[39m, in \u001b[36mTortoise.synthesize\u001b[39m\u001b[34m(self, text, config, speaker_id, voice_dirs, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    537\u001b[39m     voice_samples, conditioning_latents = load_voice(speaker_id)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.inference_with_config(\n\u001b[32m    540\u001b[39m     text, config, voice_samples=voice_samples, conditioning_latents=conditioning_latents, **kwargs\n\u001b[32m    541\u001b[39m )\n\u001b[32m    543\u001b[39m return_dict = {\n\u001b[32m    544\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    545\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdeterministic_seed\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mdeterministic_seed\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    548\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconditioning_latents\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mconditioning_latents\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    549\u001b[39m }\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m return_dict\n",
      "\u001b[31mTypeError\u001b[39m: TTS.tts.models.tortoise.Tortoise.inference_with_config() got multiple values for keyword argument 'voice_samples'"
     ]
    }
   ],
   "source": [
    "tortoise.tts_to_file(\n",
    "    sample_text,\n",
    "    file_path=\"trump_output_tortoise.wav\",\n",
    "    voice_samples=voice_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:25<00:00,  3.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:45<00:00,  3.31s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:25<00:00,  3.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [01:47<00:00,  3.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:27<00:00,  3.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [02:07<00:00,  3.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:22<00:00,  4.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:43<00:00,  3.44s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/coqpit/coqpit.py:864: UserWarning: Type mismatch in FreeVCConfig\n",
      "Failed to deserialize field: test_sentences (list[str]) = [[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"], ['Be a voice, not an echo.'], [\"I'm sorry Dave. I'm afraid I can't do that.\"], [\"This cake is great. It's so delicious and moist.\"], ['Prior to November 22, 1963.']]\n",
      "Replaced it with field's default value: []\n",
      "  self.deserialize(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trump_output.wav'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/bark\").to(device)\n",
    "tts.tts_with_vc_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=\"trump_sample.wav\",\n",
    "    file_path=\"trump_output_bark.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tts_models/multilingual/multi-dataset/xtts_v2', 'tts_models/multilingual/multi-dataset/xtts_v1.1', 'tts_models/multilingual/multi-dataset/your_tts', 'tts_models/multilingual/multi-dataset/bark', 'tts_models/bg/cv/vits', 'tts_models/cs/cv/vits', 'tts_models/da/cv/vits', 'tts_models/et/cv/vits', 'tts_models/ga/cv/vits', 'tts_models/en/ek1/tacotron2', 'tts_models/en/ljspeech/tacotron2-DDC', 'tts_models/en/ljspeech/tacotron2-DDC_ph', 'tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/speedy-speech', 'tts_models/en/ljspeech/tacotron2-DCA', 'tts_models/en/ljspeech/vits', 'tts_models/en/ljspeech/vits--neon', 'tts_models/en/ljspeech/fast_pitch', 'tts_models/en/ljspeech/overflow', 'tts_models/en/ljspeech/neural_hmm', 'tts_models/en/vctk/vits', 'tts_models/en/vctk/fast_pitch', 'tts_models/en/sam/tacotron-DDC', 'tts_models/en/blizzard2013/capacitron-t2-c50', 'tts_models/en/blizzard2013/capacitron-t2-c150_v2', 'tts_models/en/multi-dataset/tortoise-v2', 'tts_models/en/jenny/jenny', 'tts_models/es/mai/tacotron2-DDC', 'tts_models/es/css10/vits', 'tts_models/fr/mai/tacotron2-DDC', 'tts_models/fr/css10/vits', 'tts_models/uk/mai/glow-tts', 'tts_models/uk/mai/vits', 'tts_models/zh-CN/baker/tacotron2-DDC-GST', 'tts_models/nl/mai/tacotron2-DDC', 'tts_models/nl/css10/vits', 'tts_models/de/thorsten/tacotron2-DCA', 'tts_models/de/thorsten/vits', 'tts_models/de/thorsten/tacotron2-DDC', 'tts_models/de/css10/vits-neon', 'tts_models/ja/kokoro/tacotron2-DDC', 'tts_models/tr/common-voice/glow-tts', 'tts_models/it/mai_female/glow-tts', 'tts_models/it/mai_female/vits', 'tts_models/it/mai_male/glow-tts', 'tts_models/it/mai_male/vits', 'tts_models/ewe/openbible/vits', 'tts_models/hau/openbible/vits', 'tts_models/lin/openbible/vits', 'tts_models/tw_akuapem/openbible/vits', 'tts_models/tw_asante/openbible/vits', 'tts_models/yor/openbible/vits', 'tts_models/hu/css10/vits', 'tts_models/el/cv/vits', 'tts_models/fi/css10/vits', 'tts_models/hr/cv/vits', 'tts_models/lt/cv/vits', 'tts_models/lv/cv/vits', 'tts_models/mt/cv/vits', 'tts_models/pl/mai_female/vits', 'tts_models/pt/cv/vits', 'tts_models/ro/cv/vits', 'tts_models/sk/cv/vits', 'tts_models/sl/cv/vits', 'tts_models/sv/cv/vits', 'tts_models/ca/custom/vits', 'tts_models/fa/custom/glow-tts', 'tts_models/fa/custom/vits-female', 'tts_models/bn/custom/vits-male', 'tts_models/bn/custom/vits-female', 'tts_models/be/common-voice/glow-tts']\n"
     ]
    }
   ],
   "source": [
    "print(tts.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_xtts.wav'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(\n",
    "    sample_text, \n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    language=\"en\",\n",
    "    file_path=\"trump_output_xtts.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vits = TTS(\"tts_models/en/ljspeech/vits\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_vits.wav'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits.tts_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=\"trump_sample.wav\",\n",
    "    file_path=\"trump_output_vits.wav\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
