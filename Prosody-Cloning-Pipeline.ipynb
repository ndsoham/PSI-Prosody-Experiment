{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Trump dataset\n",
    "from datasets import load_dataset\n",
    "trump_dataset = load_dataset(\"tuenguyen/trump-speech-dataset-tts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to sample\n",
    "from IPython.display import Audio\n",
    "trump_sample = trump_dataset[\"train\"][0]\n",
    "audio_array = trump_sample[\"path\"][\"array\"]\n",
    "transcript = trump_sample[\"transcript\"]\n",
    "sr = trump_sample[\"path\"][\"sampling_rate\"]\n",
    "audio_data = Audio(audio_array, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the audio data to a wav file\n",
    "with open(\"trump_sample.wav\", \"wb\") as opt_file:\n",
    "    opt_file.write(audio_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "Marijuana legalization presents an opportunity to promote public health, reduce crime, and generate tax revenue. \n",
    "Evidence suggests that regulation can ensure safer consumption, reduce the burden on the criminal justice system, and direct law enforcement resources toward more serious crimes. \n",
    "Additionally, the economic benefits of legal cannabis industries, including job creation and tax revenue, have been demonstrated in states where it is already legal. \n",
    "By focusing on regulation over prohibition, we can ensure a more effective and balanced approach to marijuana use and its societal impact.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tortoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#initialize TTS\n",
    "tortoise = TTS(\"tts_models/en/multi-dataset/tortoise-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0449, -0.0230,  0.1235,  ...,  0.2088,  0.2020,  0.1975],\n",
      "       dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "voice_samples = [torch.tensor(audio_array)]\n",
    "print(voice_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TTS.tts.models.tortoise.Tortoise.inference_with_config() got multiple values for keyword argument 'voice_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtortoise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_output_tortoise.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoice_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvoice_samples\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:374\u001b[39m, in \u001b[36mTTS.tts_to_file\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, pipe_out, file_path, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    346\u001b[39m \n\u001b[32m    347\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.synthesizer.save_wav(wav=wav, path=file_path, pipe_out=pipe_out)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:323\u001b[39m, in \u001b[36mTTS.tts\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m    302\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, emotion=emotion, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/utils/synthesizer.py:422\u001b[39m, in \u001b[36mSynthesizer.tts\u001b[39m\u001b[34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.tts_model, \u001b[33m\"\u001b[39m\u001b[33msynthesize\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvoice_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43md_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    433\u001b[39m         \u001b[38;5;66;03m# synthesize voice\u001b[39;00m\n\u001b[32m    434\u001b[39m         outputs = synthesis(\n\u001b[32m    435\u001b[39m             model=\u001b[38;5;28mself\u001b[39m.tts_model,\n\u001b[32m    436\u001b[39m             text=sen,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m             language_id=language_id,\n\u001b[32m    445\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/models/tortoise.py:539\u001b[39m, in \u001b[36mTortoise.synthesize\u001b[39m\u001b[34m(self, text, config, speaker_id, voice_dirs, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    537\u001b[39m     voice_samples, conditioning_latents = load_voice(speaker_id)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.inference_with_config(\n\u001b[32m    540\u001b[39m     text, config, voice_samples=voice_samples, conditioning_latents=conditioning_latents, **kwargs\n\u001b[32m    541\u001b[39m )\n\u001b[32m    543\u001b[39m return_dict = {\n\u001b[32m    544\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    545\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdeterministic_seed\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mdeterministic_seed\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    548\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconditioning_latents\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[33m\"\u001b[39m\u001b[33mconditioning_latents\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    549\u001b[39m }\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m return_dict\n",
      "\u001b[31mTypeError\u001b[39m: TTS.tts.models.tortoise.Tortoise.inference_with_config() got multiple values for keyword argument 'voice_samples'"
     ]
    }
   ],
   "source": [
    "tortoise.tts_to_file(\n",
    "    sample_text,\n",
    "    file_path=\"trump_output_tortoise.wav\",\n",
    "    voice_samples=voice_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/bark\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 7/100 [00:02<00:25,  3.68it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_sample.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_output_bark.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:374\u001b[39m, in \u001b[36mTTS.tts_to_file\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, pipe_out, file_path, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    346\u001b[39m \n\u001b[32m    347\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.synthesizer.save_wav(wav=wav, path=file_path, pipe_out=pipe_out)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/api.py:323\u001b[39m, in \u001b[36mTTS.tts\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m    302\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, emotion=emotion, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/utils/synthesizer.py:422\u001b[39m, in \u001b[36mSynthesizer.tts\u001b[39m\u001b[34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.tts_model, \u001b[33m\"\u001b[39m\u001b[33msynthesize\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvoice_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43md_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    433\u001b[39m         \u001b[38;5;66;03m# synthesize voice\u001b[39;00m\n\u001b[32m    434\u001b[39m         outputs = synthesis(\n\u001b[32m    435\u001b[39m             model=\u001b[38;5;28mself\u001b[39m.tts_model,\n\u001b[32m    436\u001b[39m             text=sen,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m             language_id=language_id,\n\u001b[32m    445\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/models/bark.py:216\u001b[39m, in \u001b[36mBark.synthesize\u001b[39m\u001b[34m(self, text, config, speaker_id, voice_dirs, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m voice_dirs = \u001b[38;5;28mself\u001b[39m._set_voice_dirs(voice_dirs)\n\u001b[32m    215\u001b[39m history_prompt = load_voice(\u001b[38;5;28mself\u001b[39m, speaker_id, voice_dirs)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m return_dict = {\n\u001b[32m    218\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m: outputs[\u001b[32m0\u001b[39m],\n\u001b[32m    219\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext_inputs\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m    220\u001b[39m }\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/models/bark.py:149\u001b[39m, in \u001b[36mBark.generate_audio\u001b[39m\u001b[34m(self, text, history_prompt, text_temp, waveform_temp, base, allow_early_stop, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_audio\u001b[39m(\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    130\u001b[39m     text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     **kwargs,\n\u001b[32m    137\u001b[39m ):\n\u001b[32m    138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate audio array from input text.\u001b[39;00m\n\u001b[32m    139\u001b[39m \n\u001b[32m    140\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        numpy audio array at sample frequency 24khz\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     x_semantic = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_to_semantic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_early_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_early_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     audio_arr, c, f = \u001b[38;5;28mself\u001b[39m.semantic_to_waveform(\n\u001b[32m    158\u001b[39m         x_semantic, history_prompt=history_prompt, temp=waveform_temp, base=base\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m audio_arr, [x_semantic, c, f]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/models/bark.py:83\u001b[39m, in \u001b[36mBark.text_to_semantic\u001b[39m\u001b[34m(self, text, history_prompt, temp, base, allow_early_stop, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_to_semantic\u001b[39m(\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     66\u001b[39m     text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     **kwargs,\n\u001b[32m     72\u001b[39m ):\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate semantic array from text.\u001b[39;00m\n\u001b[32m     74\u001b[39m \n\u001b[32m     75\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m \u001b[33;03m        numpy semantic array to be fed into `semantic_to_waveform`\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     x_semantic = \u001b[43mgenerate_text_semantic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_early_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_early_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x_semantic\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/layers/bark/inference_funcs.py:244\u001b[39m, in \u001b[36mgenerate_text_semantic\u001b[39m\u001b[34m(text, model, history_prompt, temp, top_k, top_p, silent, min_eos_p, max_gen_duration_s, allow_early_stop, base, use_kv_caching, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m     x_input = x\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m logits, kv_cache = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msemantic_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_kv_caching\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m relevant_logits = logits[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, : model.config.SEMANTIC_VOCAB_SIZE]\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_early_stop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/layers/bark/model.py:212\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, merge_context, past_kv, position_ids, use_cache)\u001b[39m\n\u001b[32m    209\u001b[39m new_kv = () \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, (block, past_layer_kv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.transformer.h, past_kv)):\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     x, kv = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_layer_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[32m    215\u001b[39m         new_kv = new_kv + (kv,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/layers/bark/model.py:119\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, past_kv, use_cache)\u001b[39m\n\u001b[32m    117\u001b[39m attn_output, prev_kvs = \u001b[38;5;28mself\u001b[39m.attn(\u001b[38;5;28mself\u001b[39m.ln_1(x), past_kv=past_kv, use_cache=use_cache)\n\u001b[32m    118\u001b[39m x = x + attn_output\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (x, prev_kvs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/coqui-ai-TTS/TTS/tts/layers/bark/model.py:100\u001b[39m, in \u001b[36mMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.gelu(x)\n\u001b[32m    102\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.c_proj(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tts.tts_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    file_path=\"trump_output_bark.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_xtts.wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(\n",
    "    sample_text, \n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    language=\"en\",\n",
    "    file_path=\"trump_output_xtts.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vits = TTS(\"tts_models/en/ljspeech/vits\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_vits.wav'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits.tts_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    file_path=\"trump_output_vits.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvevo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvevo_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvevo_tts\u001b[39m(\n\u001b[32m      7\u001b[39m     src_text,\n\u001b[32m      8\u001b[39m     ref_wav_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     ref_language=\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m ):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timbre_ref_wav_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from models.vc.vevo.vevo_utils import *\n",
    "\n",
    "def vevo_tts(\n",
    "    src_text,\n",
    "    ref_wav_path,\n",
    "    timbre_ref_wav_path=None,\n",
    "    output_path=None,\n",
    "    ref_text=None,\n",
    "    src_language=\"en\",\n",
    "    ref_language=\"en\"\n",
    "):\n",
    "    if timbre_ref_wav_path is None:\n",
    "        timbre_ref_wav_path = ref_wav_path\n",
    "    \n",
    "    gen_audio = inference_pipeline.inference_ar_and_fm(\n",
    "        src_wav_path=None,\n",
    "        src_text=src_text,\n",
    "        style_ref_wav_path=ref_wav_path,\n",
    "        timbre_ref_wav_path=timbre_ref_wav_path,\n",
    "        style_ref_wav_text=ref_text,\n",
    "        src_text_language=src_language,\n",
    "        style_ref_wav_text_language=ref_language,\n",
    "    )\n",
    "    \n",
    "    assert output_path is not None\n",
    "    save_audio(gen_audio, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7722d542d550433ead6dcfa3e1055e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## content style tokenizer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"tokenizer/vq8192/*\"]\n",
    ")\n",
    "\n",
    "content_style_tokenizer_ckpt_path = os.path.join(local_dir, \"tokenizer/vq8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db1c4a953e046f4b1d018adc6d748f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## autoregressive transformer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"contentstyle_modeling/PhoneToVq8192/*\"],\n",
    ")\n",
    "\n",
    "ar_cfg_path = \"./models/vc/vevo/config/PhoneToVq8192.json\"\n",
    "ar_ckpt_path = os.path.join(local_dir, \"contentstyle_modeling/PhoneToVq8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5969adc0474f3b864bfcb6d704c8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## flow matching transformer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"acoustic_modeling/Vq8192ToMels/*\"],\n",
    ")\n",
    "\n",
    "fmt_cfg_path = \"./models/vc/vevo/config/Vq8192ToMels.json\"\n",
    "fmt_ckpt_path = os.path.join(local_dir, \"acoustic_modeling/Vq8192ToMels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74406841eaf84d0d8471660a29bb705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## vocoder\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"acoustic_modeling/Vocoder/*\"],\n",
    ")\n",
    "\n",
    "vocoder_cfg_path = \"./models/vc/vevo/config/Vocoder.json\"\n",
    "vocoder_ckpt_path = os.path.join(local_dir, \"acoustic_modeling/Vocoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params of AR model: 743.30 M\n",
      "#Params of Flow Matching model: 337.69 M\n",
      "#Params of Vocoder model: 255.04 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params of Content-Style Tokenizer: 44.29 M\n"
     ]
    }
   ],
   "source": [
    "inference_pipeline = VevoInferencePipeline(\n",
    "    content_style_tokenizer_ckpt_path=content_style_tokenizer_ckpt_path,\n",
    "    ar_cfg_path=ar_cfg_path,\n",
    "    ar_ckpt_path=ar_ckpt_path,\n",
    "    fmt_cfg_path=fmt_cfg_path,\n",
    "    fmt_ckpt_path=fmt_ckpt_path,\n",
    "    vocoder_cfg_path=vocoder_cfg_path,\n",
    "    vocoder_ckpt_path=vocoder_ckpt_path,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'setLangfilters' from 'LangSegment.LangSegment' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/LangSegment.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Zero-shot TTS (sample style and timbre reference)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvevo_tts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_sample.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_output_vevo.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_language\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_language\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mvevo_tts\u001b[39m\u001b[34m(src_text, ref_wav_path, timbre_ref_wav_path, output_path, ref_text, src_language, ref_language)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timbre_ref_wav_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     16\u001b[39m     timbre_ref_wav_path = ref_wav_path\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m gen_audio = \u001b[43minference_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_ar_and_fm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_wav_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimbre_ref_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimbre_ref_wav_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_text_language\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_text_language\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     29\u001b[39m save_audio(gen_audio, output_path=output_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/vc/vevo/vevo_utils.py:466\u001b[39m, in \u001b[36mVevoInferencePipeline.inference_ar_and_fm\u001b[39m\u001b[34m(self, src_wav_path, src_text, style_ref_wav_path, timbre_ref_wav_path, style_ref_wav_text, src_text_language, style_ref_wav_text_language, vc_input_mask_ratio, use_global_guided_inference, flow_matching_steps, display_audio)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m## AR ##\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task == \u001b[33m\"\u001b[39m\u001b[33mtts\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     ar_input_ids = \u001b[43mg2p_\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_text_language\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    467\u001b[39m     ar_input_ids = torch.tensor([ar_input_ids], dtype=torch.long).to(\n\u001b[32m    468\u001b[39m         \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m display_audio:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/vc/vevo/vevo_utils.py:27\u001b[39m, in \u001b[36mg2p_\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg2p_\u001b[39m(text, language):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p_generation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m g2p, chn_eng_g2p\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mzh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m chn_eng_g2p(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/tts/maskgct/g2p/g2p_generation.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PhonemeBpeTokenizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m phonemizer_g2p\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/tts/maskgct/g2p/g2p/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_tokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLangSegment\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLangSegment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangSegment,getTexts,classify,getCounts,printList,setLangfilters,getLangfilters,setfilters,getfilters\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# release\u001b[39;00m\n\u001b[32m      4\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m0.2.0\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'setLangfilters' from 'LangSegment.LangSegment' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/LangSegment.py)"
     ]
    }
   ],
   "source": [
    "### Zero-shot TTS (sample style and timbre reference)\n",
    "vevo_tts(\n",
    "    sample_text,\n",
    "    \"trump_sample.wav\",\n",
    "    output_path=\"trump_output_vevo.wav\",\n",
    "    ref_text=transcript,\n",
    "    src_language=\"en\",\n",
    "    ref_language=\"en\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
