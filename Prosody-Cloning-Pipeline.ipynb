{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Trump dataset\n",
    "from datasets import load_dataset\n",
    "trump_dataset = load_dataset(\"tuenguyen/trump-speech-dataset-tts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to sample\n",
    "from IPython.display import Audio\n",
    "trump_sample = trump_dataset[\"train\"][0]\n",
    "audio_array = trump_sample[\"path\"][\"array\"]\n",
    "transcript = trump_sample[\"transcript\"]\n",
    "sr = trump_sample[\"path\"][\"sampling_rate\"]\n",
    "audio_data = Audio(audio_array, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the audio data to a wav file\n",
    "with open(\"trump_sample.wav\", \"wb\") as opt_file:\n",
    "    opt_file.write(audio_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "Marijuana legalization presents an opportunity to promote public health, reduce crime, and generate tax revenue. \n",
    "Evidence suggests that regulation can ensure safer consumption, reduce the burden on the criminal justice system, and direct law enforcement resources toward more serious crimes. \n",
    "Additionally, the economic benefits of legal cannabis industries, including job creation and tax revenue, have been demonstrated in states where it is already legal. \n",
    "By focusing on regulation over prohibition, we can ensure a more effective and balanced approach to marijuana use and its societal impact.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/bark\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.91it/s]\n",
      "100%|██████████| 23/23 [01:13<00:00,  3.20s/it]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n",
      "100%|██████████| 31/31 [01:48<00:00,  3.51s/it]\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.12it/s]\n",
      "100%|██████████| 26/26 [01:30<00:00,  3.46s/it]\n",
      "100%|██████████| 100/100 [00:22<00:00,  4.41it/s]\n",
      "100%|██████████| 29/29 [01:43<00:00,  3.56s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/coqpit/coqpit.py:864: UserWarning: Type mismatch in FreeVCConfig\n",
      "Failed to deserialize field: test_sentences (list[str]) = [[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"], ['Be a voice, not an echo.'], [\"I'm sorry Dave. I'm afraid I can't do that.\"], [\"This cake is great. It's so delicious and moist.\"], ['Prior to November 22, 1963.']]\n",
      "Replaced it with field's default value: []\n",
      "  self.deserialize(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trump_output_bark.wav'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_with_vc_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    file_path=\"trump_output_bark.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTS V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > You must confirm the following:\n",
      " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
      " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.02G/1.02G [2:37:51<00:00, 107kiB/s]\n",
      "100%|██████████| 1.87G/1.87G [01:08<00:00, 27.3MiB/s]\n",
      "100%|██████████| 4.70k/4.70k [00:00<00:00, 31.6kiB/s]\n",
      "100%|██████████| 294k/294k [00:00<00:00, 977kiB/s]\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "xttsv1 = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1.1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tts_models/multilingual/multi-dataset/xtts_v2',\n",
       " 'tts_models/multilingual/multi-dataset/xtts_v1.1',\n",
       " 'tts_models/multilingual/multi-dataset/your_tts',\n",
       " 'tts_models/multilingual/multi-dataset/bark',\n",
       " 'tts_models/bg/cv/vits',\n",
       " 'tts_models/cs/cv/vits',\n",
       " 'tts_models/da/cv/vits',\n",
       " 'tts_models/et/cv/vits',\n",
       " 'tts_models/ga/cv/vits',\n",
       " 'tts_models/en/ek1/tacotron2',\n",
       " 'tts_models/en/ljspeech/tacotron2-DDC',\n",
       " 'tts_models/en/ljspeech/tacotron2-DDC_ph',\n",
       " 'tts_models/en/ljspeech/glow-tts',\n",
       " 'tts_models/en/ljspeech/speedy-speech',\n",
       " 'tts_models/en/ljspeech/tacotron2-DCA',\n",
       " 'tts_models/en/ljspeech/vits',\n",
       " 'tts_models/en/ljspeech/vits--neon',\n",
       " 'tts_models/en/ljspeech/fast_pitch',\n",
       " 'tts_models/en/ljspeech/overflow',\n",
       " 'tts_models/en/ljspeech/neural_hmm',\n",
       " 'tts_models/en/vctk/vits',\n",
       " 'tts_models/en/vctk/fast_pitch',\n",
       " 'tts_models/en/sam/tacotron-DDC',\n",
       " 'tts_models/en/blizzard2013/capacitron-t2-c50',\n",
       " 'tts_models/en/blizzard2013/capacitron-t2-c150_v2',\n",
       " 'tts_models/en/multi-dataset/tortoise-v2',\n",
       " 'tts_models/en/jenny/jenny',\n",
       " 'tts_models/es/mai/tacotron2-DDC',\n",
       " 'tts_models/es/css10/vits',\n",
       " 'tts_models/fr/mai/tacotron2-DDC',\n",
       " 'tts_models/fr/css10/vits',\n",
       " 'tts_models/uk/mai/glow-tts',\n",
       " 'tts_models/uk/mai/vits',\n",
       " 'tts_models/zh-CN/baker/tacotron2-DDC-GST',\n",
       " 'tts_models/nl/mai/tacotron2-DDC',\n",
       " 'tts_models/nl/css10/vits',\n",
       " 'tts_models/de/thorsten/tacotron2-DCA',\n",
       " 'tts_models/de/thorsten/vits',\n",
       " 'tts_models/de/thorsten/tacotron2-DDC',\n",
       " 'tts_models/de/css10/vits-neon',\n",
       " 'tts_models/ja/kokoro/tacotron2-DDC',\n",
       " 'tts_models/tr/common-voice/glow-tts',\n",
       " 'tts_models/it/mai_female/glow-tts',\n",
       " 'tts_models/it/mai_female/vits',\n",
       " 'tts_models/it/mai_male/glow-tts',\n",
       " 'tts_models/it/mai_male/vits',\n",
       " 'tts_models/ewe/openbible/vits',\n",
       " 'tts_models/hau/openbible/vits',\n",
       " 'tts_models/lin/openbible/vits',\n",
       " 'tts_models/tw_akuapem/openbible/vits',\n",
       " 'tts_models/tw_asante/openbible/vits',\n",
       " 'tts_models/yor/openbible/vits',\n",
       " 'tts_models/hu/css10/vits',\n",
       " 'tts_models/el/cv/vits',\n",
       " 'tts_models/fi/css10/vits',\n",
       " 'tts_models/hr/cv/vits',\n",
       " 'tts_models/lt/cv/vits',\n",
       " 'tts_models/lv/cv/vits',\n",
       " 'tts_models/mt/cv/vits',\n",
       " 'tts_models/pl/mai_female/vits',\n",
       " 'tts_models/pt/cv/vits',\n",
       " 'tts_models/ro/cv/vits',\n",
       " 'tts_models/sk/cv/vits',\n",
       " 'tts_models/sl/cv/vits',\n",
       " 'tts_models/sv/cv/vits',\n",
       " 'tts_models/ca/custom/vits',\n",
       " 'tts_models/fa/custom/glow-tts',\n",
       " 'tts_models/fa/custom/vits-female',\n",
       " 'tts_models/bn/custom/vits-male',\n",
       " 'tts_models/bn/custom/vits-female',\n",
       " 'tts_models/be/common-voice/glow-tts',\n",
       " 'vocoder_models/universal/libri-tts/wavegrad',\n",
       " 'vocoder_models/universal/libri-tts/fullband-melgan',\n",
       " 'vocoder_models/en/ek1/wavegrad',\n",
       " 'vocoder_models/en/librispeech100/wavlm-hifigan',\n",
       " 'vocoder_models/en/librispeech100/wavlm-hifigan_prematched',\n",
       " 'vocoder_models/en/ljspeech/multiband-melgan',\n",
       " 'vocoder_models/en/ljspeech/hifigan_v2',\n",
       " 'vocoder_models/en/ljspeech/univnet',\n",
       " 'vocoder_models/en/blizzard2013/hifigan_v2',\n",
       " 'vocoder_models/en/vctk/hifigan_v2',\n",
       " 'vocoder_models/en/sam/hifigan_v2',\n",
       " 'vocoder_models/nl/mai/parallel-wavegan',\n",
       " 'vocoder_models/de/thorsten/wavegrad',\n",
       " 'vocoder_models/de/thorsten/fullband-melgan',\n",
       " 'vocoder_models/de/thorsten/hifigan_v1',\n",
       " 'vocoder_models/ja/kokoro/hifigan_v1',\n",
       " 'vocoder_models/uk/mai/multiband-melgan',\n",
       " 'vocoder_models/tr/common-voice/hifigan',\n",
       " 'vocoder_models/be/common-voice/hifigan',\n",
       " 'voice_conversion_models/multilingual/vctk/freevc24',\n",
       " 'voice_conversion_models/multilingual/multi-dataset/knnvc',\n",
       " 'voice_conversion_models/multilingual/multi-dataset/openvoice_v1',\n",
       " 'voice_conversion_models/multilingual/multi-dataset/openvoice_v2']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xttsv1.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_xttsv1.wav'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xttsv1.tts_to_file(\n",
    "    sample_text, \n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    language=\"en\",\n",
    "    file_path=\"trump_output_xttsv1.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XTTS V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump_output_xttsv2.wav'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(\n",
    "    sample_text, \n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    language=\"en\",\n",
    "    file_path=\"trump_output_xttsv2.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/coqpit/coqpit.py:864: UserWarning: Type mismatch in FreeVCConfig\n",
      "Failed to deserialize field: test_sentences (list[str]) = [[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"], ['Be a voice, not an echo.'], [\"I'm sorry Dave. I'm afraid I can't do that.\"], [\"This cake is great. It's so delicious and moist.\"], ['Prior to November 22, 1963.']]\n",
      "Replaced it with field's default value: []\n",
      "  self.deserialize(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trump_output_taco2.wav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tactron2 = TTS(\"tts_models/en/ljspeech/tacotron2-DDC\")\n",
    "tactron2.tts_with_vc_to_file(\n",
    "    sample_text,\n",
    "    speaker_wav=[\"trump_sample.wav\"],\n",
    "    file_path=\"trump_output_taco2.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvevo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvevo_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvevo_tts\u001b[39m(\n\u001b[32m      7\u001b[39m     src_text,\n\u001b[32m      8\u001b[39m     ref_wav_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     ref_language=\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m ):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timbre_ref_wav_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from models.vc.vevo.vevo_utils import *\n",
    "\n",
    "def vevo_tts(\n",
    "    src_text,\n",
    "    ref_wav_path,\n",
    "    timbre_ref_wav_path=None,\n",
    "    output_path=None,\n",
    "    ref_text=None,\n",
    "    src_language=\"en\",\n",
    "    ref_language=\"en\"\n",
    "):\n",
    "    if timbre_ref_wav_path is None:\n",
    "        timbre_ref_wav_path = ref_wav_path\n",
    "    \n",
    "    gen_audio = inference_pipeline.inference_ar_and_fm(\n",
    "        src_wav_path=None,\n",
    "        src_text=src_text,\n",
    "        style_ref_wav_path=ref_wav_path,\n",
    "        timbre_ref_wav_path=timbre_ref_wav_path,\n",
    "        style_ref_wav_text=ref_text,\n",
    "        src_text_language=src_language,\n",
    "        style_ref_wav_text_language=ref_language,\n",
    "    )\n",
    "    \n",
    "    assert output_path is not None\n",
    "    save_audio(gen_audio, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7722d542d550433ead6dcfa3e1055e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## content style tokenizer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"tokenizer/vq8192/*\"]\n",
    ")\n",
    "\n",
    "content_style_tokenizer_ckpt_path = os.path.join(local_dir, \"tokenizer/vq8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db1c4a953e046f4b1d018adc6d748f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## autoregressive transformer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"contentstyle_modeling/PhoneToVq8192/*\"],\n",
    ")\n",
    "\n",
    "ar_cfg_path = \"./models/vc/vevo/config/PhoneToVq8192.json\"\n",
    "ar_ckpt_path = os.path.join(local_dir, \"contentstyle_modeling/PhoneToVq8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5969adc0474f3b864bfcb6d704c8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## flow matching transformer\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"acoustic_modeling/Vq8192ToMels/*\"],\n",
    ")\n",
    "\n",
    "fmt_cfg_path = \"./models/vc/vevo/config/Vq8192ToMels.json\"\n",
    "fmt_ckpt_path = os.path.join(local_dir, \"acoustic_modeling/Vq8192ToMels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74406841eaf84d0d8471660a29bb705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## vocoder\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"amphion/Vevo\",\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\"./ckpts/Vevo\",\n",
    "    allow_patterns=[\"acoustic_modeling/Vocoder/*\"],\n",
    ")\n",
    "\n",
    "vocoder_cfg_path = \"./models/vc/vevo/config/Vocoder.json\"\n",
    "vocoder_ckpt_path = os.path.join(local_dir, \"acoustic_modeling/Vocoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params of AR model: 743.30 M\n",
      "#Params of Flow Matching model: 337.69 M\n",
      "#Params of Vocoder model: 255.04 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params of Content-Style Tokenizer: 44.29 M\n"
     ]
    }
   ],
   "source": [
    "inference_pipeline = VevoInferencePipeline(\n",
    "    content_style_tokenizer_ckpt_path=content_style_tokenizer_ckpt_path,\n",
    "    ar_cfg_path=ar_cfg_path,\n",
    "    ar_ckpt_path=ar_ckpt_path,\n",
    "    fmt_cfg_path=fmt_cfg_path,\n",
    "    fmt_ckpt_path=fmt_ckpt_path,\n",
    "    vocoder_cfg_path=vocoder_cfg_path,\n",
    "    vocoder_ckpt_path=vocoder_ckpt_path,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'setLangfilters' from 'LangSegment.LangSegment' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/LangSegment.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Zero-shot TTS (sample style and timbre reference)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvevo_tts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_sample.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrump_output_vevo.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_language\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_language\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mvevo_tts\u001b[39m\u001b[34m(src_text, ref_wav_path, timbre_ref_wav_path, output_path, ref_text, src_language, ref_language)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timbre_ref_wav_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     16\u001b[39m     timbre_ref_wav_path = ref_wav_path\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m gen_audio = \u001b[43minference_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference_ar_and_fm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_wav_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimbre_ref_wav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimbre_ref_wav_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_text_language\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_ref_wav_text_language\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     29\u001b[39m save_audio(gen_audio, output_path=output_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/vc/vevo/vevo_utils.py:466\u001b[39m, in \u001b[36mVevoInferencePipeline.inference_ar_and_fm\u001b[39m\u001b[34m(self, src_wav_path, src_text, style_ref_wav_path, timbre_ref_wav_path, style_ref_wav_text, src_text_language, style_ref_wav_text_language, vc_input_mask_ratio, use_global_guided_inference, flow_matching_steps, display_audio)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m## AR ##\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task == \u001b[33m\"\u001b[39m\u001b[33mtts\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     ar_input_ids = \u001b[43mg2p_\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_text_language\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    467\u001b[39m     ar_input_ids = torch.tensor([ar_input_ids], dtype=torch.long).to(\n\u001b[32m    468\u001b[39m         \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m display_audio:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/vc/vevo/vevo_utils.py:27\u001b[39m, in \u001b[36mg2p_\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg2p_\u001b[39m(text, language):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p_generation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m g2p, chn_eng_g2p\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mzh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m chn_eng_g2p(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/tts/maskgct/g2p/g2p_generation.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PhonemeBpeTokenizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m phonemizer_g2p\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Texas A&M University/Spring 2025/PSI/PSI-Prosody-Experiment/Amphion/models/tts/maskgct/g2p/g2p/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskgct\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_tokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLangSegment\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLangSegment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangSegment,getTexts,classify,getCounts,printList,setLangfilters,getLangfilters,setfilters,getfilters\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# release\u001b[39;00m\n\u001b[32m      4\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m0.2.0\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'setLangfilters' from 'LangSegment.LangSegment' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/LangSegment/LangSegment.py)"
     ]
    }
   ],
   "source": [
    "### Zero-shot TTS (sample style and timbre reference)\n",
    "vevo_tts(\n",
    "    sample_text,\n",
    "    \"trump_sample.wav\",\n",
    "    output_path=\"trump_output_vevo.wav\",\n",
    "    ref_text=transcript,\n",
    "    src_language=\"en\",\n",
    "    ref_language=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import BaseSpeakerTTS, ToneColorConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/base_speakers/EN/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "Loaded checkpoint 'checkpoints/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    }
   ],
   "source": [
    "## init\n",
    "ckpt_base = 'checkpoints/base_speakers/EN'\n",
    "ckpt_converter = 'checkpoints/converter'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_dir = \"/\"\n",
    "\n",
    "base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)\n",
    "base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain tone color embedding\n",
    "source_se = torch.load(f'{ckpt_base}/en_default_se.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v1\n",
      "[(0.0, 12.242), (12.526, 15.058), (15.118, 20.69), (21.23, 31.4)]\n",
      "after vad: dur = 30.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/functional.py:709: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/SpectralOps.cpp:878.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "reference_speaker = 'trump_sample.wav' # This is the voice you want to clone\n",
    "target_se, audio_name = se_extractor.get_se(reference_speaker, tone_color_converter, target_dir='processed', vad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "Marijuana legalization presents an opportunity to promote public health, reduce crime,\n",
      "and generate tax revenue. Evidence suggests that regulation can ensure safer consumption,\n",
      "reduce the burden on the criminal justice system, and direct law enforcement resources toward more serious crimes.\n",
      "Additionally, the economic benefits of legal cannabis industries, including job creation and tax revenue,\n",
      "have been demonstrated in states where it is already legal. By focusing on regulation over prohibition,\n",
      "we can ensure a more effective and balanced approach to marijuana use and its societal impact.\n",
      " > ===========================\n",
      "ˌmɛɹəˈwɑnə ˌligələˈzeɪʃən ˈpɹɛzənts ən ˌɑpəɹˈtunəti tɪ pɹəˈmoʊt ˈpəblɪk hɛɫθ, ɹɪˈdus kɹaɪm,\n",
      " length:91\n",
      " length:91\n",
      "ənd ˈdʒɛnəɹˌeɪt tæks ˈɹɛvəˌnu. ˈɛvədəns səˈdʒɛsts ðət ˌɹɛgjəˈleɪʃən kən ɪnˈʃʊɹ ˈseɪfəɹ kənˈsəmʃən,\n",
      " length:98\n",
      " length:98\n",
      "ɹɪˈdus ðə ˈbəɹdən ɔn ðə ˈkɹɪmənəɫ ˈdʒəstɪs ˈsɪstəm, ənd dɪˈɹɛkt lɔ ɛnˈfɔɹsmənt ˈɹisɔɹsɪz təˈwɔɹd mɔɹ ˈsɪɹiəs kɹaɪmz.\n",
      " length:116\n",
      " length:116\n",
      "əˈdɪʃəˌnəli, ðə ˌɛkəˈnɑmɪk ˈbɛnəfɪts əv ˈligəɫ ˈkænəbəs ˈɪndəstɹiz, ˌɪnˈkludɪŋ dʒɑb kɹiˈeɪʃən ənd tæks ˈɹɛvəˌnu,\n",
      " length:112\n",
      " length:112\n",
      "hæv bɪn ˈdɛmənˌstɹeɪtɪd ɪn steɪts wɛɹ ɪt ɪz ɔˈɹɛdi ˈligəɫ. baɪ ˈfoʊkɪsɪŋ ɔn ˌɹɛgjəˈleɪʃən ˈoʊvəɹ ˌpɹoʊəˈbɪʃən,\n",
      " length:110\n",
      " length:110\n",
      "wi kən ɪnˈʃʊɹ ə mɔɹ ˈifɛktɪv ənd ˈbælənst əˈpɹoʊtʃ tɪ ˌmɛɹəˈwɑnə juz ənd ɪts səˈsaɪɪtəɫ ˌɪmˈpækt.\n",
      " length:97\n",
      " length:97\n"
     ]
    }
   ],
   "source": [
    "save_path = \"trump_output_openvoice.wav\"\n",
    "\n",
    "# run the base speaker tts\n",
    "src_path = \"tmp.wav\"\n",
    "base_speaker_tts.tts(sample_text, src_path, speaker=\"shouting\", language=\"English\", speed=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path,\n",
    "    src_se=source_se,\n",
    "    tgt_se=target_se,\n",
    "    output_path=save_path,\n",
    "    message=encode_message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
